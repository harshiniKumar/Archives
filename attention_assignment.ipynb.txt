{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanisms & Modern LLM Architectures\n",
    "## Programming Assignment\n",
    "\n",
    "**Course:** Understanding Attention Mechanisms  \n",
    "**Total Points:** 100  \n",
    "**Due Date:** [To be specified by instructor]\n",
    "\n",
    "---\n",
    "\n",
    "### Instructions:\n",
    "- Complete all code cells marked with `# YOUR CODE HERE`\n",
    "- Do not modify the test cells or grader code\n",
    "- Submit your completed notebook\n",
    "- Ensure all cells run without errors before submission\n",
    "\n",
    "### Grading Breakdown:\n",
    "- Part 1: Tokenization (25 points)\n",
    "- Part 2: Attention Mechanism (30 points)\n",
    "- Part 3: Multi-Head Attention (25 points)\n",
    "- Part 4: Optimizations & Analysis (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Run this cell first\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete! Student ID:\", hashlib.md5(b\"student\").hexdigest()[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Tokenization (25 points)\n",
    "\n",
    "In this section, you'll implement a simplified Byte-Pair Encoding (BPE) tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1: Character-Level Tokenization (5 points)\n",
    "\n",
    "Implement a function that tokenizes text at the character level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize text at character level.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "    \n",
    "    Returns:\n",
    "        list: List of character tokens\n",
    "    \n",
    "    Example:\n",
    "        >>> char_tokenize(\"hello\")\n",
    "        ['h', 'e', 'l', 'l', 'o']\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# GRADER CELL - DO NOT MODIFY\n",
    "def _grade_1_1():\n",
    "    score = 0\n",
    "    try:\n",
    "        # Test 1: Basic functionality\n",
    "        result = char_tokenize(\"hello\")\n",
    "        if result == ['h', 'e', 'l', 'l', 'o']:\n",
    "            score += 2\n",
    "        \n",
    "        # Test 2: With spaces\n",
    "        result = char_tokenize(\"hi there\")\n",
    "        if result == ['h', 'i', ' ', 't', 'h', 'e', 'r', 'e']:\n",
    "            score += 2\n",
    "        \n",
    "        # Test 3: Empty string\n",
    "        result = char_tokenize(\"\")\n",
    "        if result == []:\n",
    "            score += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    return score\n",
    "\n",
    "_q1_1_score = _grade_1_1()\n",
    "print(f\"Question 1.1 Score: {_q1_1_score}/5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2: Find Most Frequent Pair (10 points)\n",
    "\n",
    "Implement the core BPE operation: finding the most frequent adjacent token pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent_pair(tokens):\n",
    "    \"\"\"\n",
    "    Find the most frequent adjacent pair in a list of tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens (list): List of tokens\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Most frequent pair (token1, token2), or None if no pairs\n",
    "    \n",
    "    Example:\n",
    "        >>> get_most_frequent_pair(['l', 'o', 'w', 'l', 'o', 'w'])\n",
    "        ('l', 'o')\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# GRADER CELL - DO NOT MODIFY\n",
    "def _grade_1_2():\n",
    "    score = 0\n",
    "    try:\n",
    "        # Test 1: Simple case\n",
    "        result = get_most_frequent_pair(['l', 'o', 'w', 'l', 'o', 'w'])\n",
    "        if result == ('l', 'o') or result == ('o', 'w'):\n",
    "            score += 3\n",
    "        \n",
    "        # Test 2: Clear winner\n",
    "        result = get_most_frequent_pair(['a', 'b', 'a', 'b', 'a', 'b', 'c', 'd'])\n",
    "        if result == ('a', 'b'):\n",
    "            score += 4\n",
    "        \n",
    "        # Test 3: Single token\n",
    "        result = get_most_frequent_pair(['x'])\n",
    "        if result is None:\n",
    "            score += 3\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    return score\n",
    "\n",
    "_q1_2_score = _grade_1_2()\n",
    "print(f\"Question 1.2 Score: {_q1_2_score}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3: Merge Token Pairs (10 points)\n",
    "\n",
    "Implement a function to merge all occurrences of a token pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(tokens, pair):\n",
    "    \"\"\"\n",
    "    Merge all occurrences of a pair of tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens (list): List of tokens\n",
    "        pair (tuple): Pair to merge (token1, token2)\n",
    "    \n",
    "    Returns:\n",
    "        list: New list with pairs merged\n",
    "    \n",
    "    Example:\n",
    "        >>> merge_pair(['l', 'o', 'w', 'l', 'o', 'w'], ('l', 'o'))\n",
    "        ['lo', 'w', 'lo', 'w']\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# GRADER CELL - DO NOT MODIFY\n",
    "def _grade_1_3():\n",
    "    score = 0\n",
    "    try:\n",
    "        # Test 1: Basic merge\n",
    "        result = merge_pair(['l', 'o', 'w'], ('l', 'o'))\n",
    "        if result == ['lo', 'w']:\n",
    "            score += 3\n",
    "        \n",
    "        # Test 2: Multiple occurrences\n",
    "        result = merge_pair(['l', 'o', 'w', 'l', 'o', 'w'], ('l', 'o'))\n",
    "        if result == ['lo', 'w', 'lo', 'w']:\n",
    "            score += 4\n",
    "        \n",
    "        # Test 3: Non-adjacent pairs shouldn't merge\n",
    "        result = merge_pair(['l', 'x', 'o', 'w'], ('l', 'o'))\n",
    "        if result == ['l', 'x', 'o', 'w']:\n",
    "            score += 3\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    return score\n",
    "\n",
    "_q1_3_score = _grade_1_3()\n",
    "print(f\"Question 1.3 Score: {_q1_3_score}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Attention Mechanism (30 points)\n",
    "\n",
    "Implement the core attention mechanism components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1: Scaled Dot-Product Attention (15 points)\n",
    "\n",
    "Implement the scaled dot-product attention mechanism:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q (np.ndarray): Query matrix of shape (seq_len, d_k)\n",
    "        K (np.ndarray): Key matrix of shape (seq_len, d_k)\n",
    "        V (np.ndarray): Value matrix of shape (seq_len, d_v)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (attention_output, attention_weights)\n",
    "            - attention_output: shape (seq_len, d_v)\n",
    "            - attention_weights: shape (seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: \n",
    "    # 1. Compute QK^T\n",
    "    # 2. Scale by sqrt(d_k)\n",
    "    # 3. Apply softmax\n",
    "    # 4. Multiply by V\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# GRADER CELL - DO NOT MODIFY\n",
    "def _grade_2_1():\n",
    "    score = 0\n",
    "    try:\n",
    "        # Test with known values\n",
    "        Q = np.array([[1, 0], [0, 1]])\n",
    "        K = np.array([[1, 0], [0, 1]])\n",
    "        V = np.array([[1, 2], [3, 4]])\n",
    "        \n",
    "        output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "        \n",
    "        # Check shapes\n",
    "        if output.shape == (2, 2) and weights.shape == (2, 2):\n",
    "            score += 5\n",
    "        \n",
    "        # Check weights sum to 1\n",
    "        if np.allclose(weights.sum(axis=1), 1.0):\n",
    "            score += 5\n",
    "        \n",
    "        # Check output values are reasonable\n",
    "        if output.shape == (2, 2) and not np.isnan(output).any():\n",
    "            score += 5\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    return score\n",
    "\n",
    "_q2_1_score = _grade_2_1()\n",
    "print(f\"Question 2.1 Score: {_q2_1_score}/15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2: Visualize Attention Weights (15 points)\n",
    "\n",
    "Create a function to visualize attention weights as a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(attention_weights, tokens):\n",
    "    \"\"\"\n",
    "    Visualize attention weights as a heatmap.\n",
    "    \n",
    "    Args:\n",
    "        attention_weights (np.ndarray): Attention weights matrix (seq_len, seq_len)\n",
    "        tokens (list): List of token strings\n",
    "    \n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: The created figure\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: Use plt.imshow() with appropriate labels\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# GRADER CELL - DO NOT MODIFY\n",
    "def _grade_2_2():\n",
    "    score = 0\n",
    "    try:\n",
    "        # Create sample attention weights\n",
    "        weights = np.array([[0.7, 0.2, 0.1],\n",
    "                           [0.1, 0.8, 0.1],\n",
    "                           [0.2, 0.3, 0.5]])\n",
    "        tokens = ['The', 'cat', 'sat']\n",
    "        \n",
    "        fig = visualize_attention(weights, tokens)\n",
    "        \n",
    "        # Check if figure was created\n",
    "        if fig is not None:\n",
    "            score += 10\n",
    "            \n",
    "            # Check if it's a matplotlib figure\n",
    "            if hasattr(fig, 'axes'):\n",
    "                score += 5\n",
    "        \n",
    "        plt.close('all')  # Clean up\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    return score\n",
    "\n",
    "_q2_2_score = _grade_2_2()\n",
    "print(f\"Question 2.2 Score: {_q2_2_score}/15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Multi-Head Attention (25 points)\n",
    "\n",
    "Implement Multi-Head Attention architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1: Split Heads (10 points)\n",
    "\n",
    "Implement the function to split embeddings into multiple attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, num_heads):\n",
    "    \"\"\"\n",
    "    Split the last dimension into (num_heads, depth).\n",
    "    \n",
    "    Args:\n",
    "        x (np.ndarray): Input tensor of shape (seq_len, d_model)\n",
    "        num_heads (int): Number of attention heads\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Reshaped tensor of shape (num_heads, seq_len, depth)\n",
    "            where depth = d_model // num_heads\n",
    "    \n",
    "    Example:\n",
    "        >>> x = np.random.randn(4, 8)  # seq_len=4, d_model=8\n",
    "        >>> result = split_heads(x, num_heads=2)\n",
    "        >>> result.shape\n",
    "        (2, 4, 4)  # (num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# GRADER CELL - DO NOT MODIFY\n",
    "def _grade_3_1():\n",
    "    score = 0\n",
    "    try:\n",
    "        # Test 1: Shape check\n",
    "        x = np.random.randn(4, 8)\n",
    "        result = split_heads(x, 2)\n",
    "        if result.shape == (2, 4, 4):\n",
    "            score += 5\n",
    "        \n",
    "        # Test 2: Different configuration\n",
    "        x = np.random.randn(6, 12)\n",
    "        result = split_heads(x, 3)\n",
    "        if result.shape == (3, 6, 4):\n",
    "            score += 5\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    return score\n",
    "\n",
    "_q3_1_score = _grade_3_1()\n",
    "print(f\"Question 3.1 Score: {_q3_1_score}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2: Calculate KV Cache Size (15 points)\n",
    "\n",
    "Calculate memory requirements for different attention mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kv_cache_size(seq_length, num_layers, d_model, num_heads, \n",
    "                           attention_type='MHA'):\n",
    "    \"\"\"\n",
    "    Calculate KV cache size in MB for different attention mechanisms.\n",
    "    \n",
    "    Args:\n",
    "        seq_length (int): Sequence length\n",
    "        num_layers (int): Number of transformer layers\n",
    "        d_model (int): Model dimension\n",
    "        num_heads (int): Number of query heads\n",
    "        attention_type (str): 'MHA', 'GQA', or 'MQA'\n",
    "            - MHA: num_kv_heads = num_heads\n",
    "            - GQA: num_kv_heads = num_heads // 4 (assume 4 queries per KV)\n",
    "            - MQA: num_kv_heads = 1\n",
    "    \n",
    "    Returns:\n",
    "        float: KV cache size in MB (assuming float32 = 4 bytes)\n",
    "    \n",
    "    Formula:\n",
    "        size = 2 (K and V) Ã— seq_length Ã— num_layers Ã— num_kv_heads Ã— (d_model/num_heads) Ã— 4 bytes\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# GRADER CELL - DO NOT MODIFY\n",
    "def _grade_3_2():\n",
    "    score = 0\n",
    "    try:\n",
    "        # Test MHA\n",
    "        mha_size = calculate_kv_cache_size(2048, 32, 4096, 32, 'MHA')\n",
    "        expected_mha = 2 * 2048 * 32 * 32 * 128 * 4 / (1024**2)  # ~1024 MB\n",
    "        if abs(mha_size - expected_mha) < 1:\n",
    "            score += 5\n",
    "        \n",
    "        # Test GQA\n",
    "        gqa_size = calculate_kv_cache_size(2048, 32, 4096, 32, 'GQA')\n",
    "        if gqa_size < mha_size and gqa_size > 0:\n",
    "            score += 5\n",
    "        \n",
    "        # Test MQA\n",
    "        mqa_size = calculate_kv_cache_size(2048, 32, 4096, 32, 'MQA')\n",
    "        if mqa_size < gqa_size and mqa_size > 0:\n",
    "            score += 5\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    return score\n",
    "\n",
    "_q3_2_score = _grade_3_2()\n",
    "print(f\"Question 3.2 Score: {_q3_2_score}/15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Analysis & Comparison (20 points)\n",
    "\n",
    "Analyze and compare different attention mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1: Attention Mechanism Comparison (10 points)\n",
    "\n",
    "Create a comparison table for different attention mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_attention_mechanisms():\n",
    "    \"\"\"\n",
    "    Return a dictionary comparing MHA, GQA, MQA, and MLA.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with keys 'MHA', 'GQA', 'MQA', 'MLA'\n",
    "              Each value should be a dict with keys:\n",
    "              - 'kv_heads': str describing number of KV heads\n",
    "              - 'speed_vs_mha': float (1.0 for MHA, >1.0 for faster)\n",
    "              - 'quality_percent': float (100.0 for MHA, <100.0 for others)\n",
    "              - 'used_by': list of model names\n",
    "    \n",
    "    Example:\n",
    "        {\n",
    "            'MHA': {\n",
    "                'kv_heads': 'H (all heads)',\n",
    "                'speed_vs_mha': 1.0,\n",
    "                'quality_percent': 100.0,\n",
    "                'used_by': ['GPT-3', 'BERT']\n",
    "            },\n",
    "            ...\n",
    "        }\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# GRADER CELL - DO NOT MODIFY\n",
    "def _grade_4_1():\n",
    "    score = 0\n",
    "    try:\n",
    "        result = compare_attention_mechanisms()\n",
    "        \n",
    "        # Check all keys present\n",
    "        if all(k in result for k in ['MHA', 'GQA', 'MQA', 'MLA']):\n",
    "            score += 2\n",
    "        \n",
    "        # Check MHA baseline\n",
    "        if result.get('MHA', {}).get('speed_vs_mha') == 1.0:\n",
    "            score += 2\n",
    "        \n",
    "        # Check GQA is faster\n",
    "        if result.get('GQA', {}).get('speed_vs_mha', 0) > 1.0:\n",
    "            score += 2\n",
    "        \n",
    "        # Check quality ordering\n",
    "        if (result.get('MHA', {}).get('quality_percent', 0) >= \n",
    "            result.get('GQA', {}).get('quality_percent', 0) >=\n",
    "            result.get('MQA', {}).get('quality_percent', 0)):\n",
    "            score += 2\n",
    "        \n",
    "        # Check model associations\n",
    "        if 'LLaMA' in str(result.get('GQA', {}).get('used_by', [])).lower():\n",
    "            score += 2\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    return score\n",
    "\n",
    "_q4_1_score = _grade_4_1()\n",
    "print(f\"Question 4.1 Score: {_q4_1_score}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2: Model Selection (10 points)\n",
    "\n",
    "Write a function to recommend the best model for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_model(use_case):\n",
    "    \"\"\"\n",
    "    Recommend the best model for a given use case.\n",
    "    \n",
    "    Args:\n",
    "        use_case (str): One of:\n",
    "            - 'multilingual': Need support for 20+ languages\n",
    "            - 'research': Need open source for fine-tuning\n",
    "            - 'efficiency': Need maximum speed and lowest memory\n",
    "            - 'math_coding': Need best performance on technical tasks\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            'model': str (model name),\n",
    "            'reason': str (why this model)\n",
    "        }\n",
    "    \n",
    "    Expected answers:\n",
    "        - multilingual â†’ Qwen2.5 (29 languages)\n",
    "        - research â†’ LLaMA 3 (open source)\n",
    "        - efficiency â†’ DeepSeek-V3 (MLA)\n",
    "        - math_coding â†’ DeepSeek-V3 (SOTA benchmarks)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# GRADER CELL - DO NOT MODIFY\n",
    "def _grade_4_2():\n",
    "    score = 0\n",
    "    try:\n",
    "        # Test multilingual\n",
    "        result = recommend_model('multilingual')\n",
    "        if 'qwen' in result.get('model', '').lower():\n",
    "            score += 3\n",
    "        \n",
    "        # Test research\n",
    "        result = recommend_model('research')\n",
    "        if 'llama' in result.get('model', '').lower():\n",
    "            score += 2\n",
    "        \n",
    "        # Test efficiency\n",
    "        result = recommend_model('efficiency')\n",
    "        if 'deepseek' in result.get('model', '').lower():\n",
    "            score += 3\n",
    "        \n",
    "        # Check reason provided\n",
    "        result = recommend_model('math_coding')\n",
    "        if len(result.get('reason', '')) > 10:\n",
    "            score += 2\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    return score\n",
    "\n",
    "_q4_2_score = _grade_4_2()\n",
    "print(f\"Question 4.2 Score: {_q4_2_score}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# FINAL GRADER - DO NOT MODIFY\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def calculate_final_score():\n",
    "    scores = {\n",
    "        'Part 1: Tokenization': {\n",
    "            'Q1.1': _q1_1_score,\n",
    "            'Q1.2': _q1_2_score,\n",
    "            'Q1.3': _q1_3_score,\n",
    "            'Total': _q1_1_score + _q1_2_score + _q1_3_score,\n",
    "            'Max': 25\n",
    "        },\n",
    "        'Part 2: Attention': {\n",
    "            'Q2.1': _q2_1_score,\n",
    "            'Q2.2': _q2_2_score,\n",
    "            'Total': _q2_1_score + _q2_2_score,\n",
    "            'Max': 30\n",
    "        },\n",
    "        'Part 3: Multi-Head': {\n",
    "            'Q3.1': _q3_1_score,\n",
    "            'Q3.2': _q3_2_score,\n",
    "            'Total': _q3_1_score + _q3_2_score,\n",
    "            'Max': 25\n",
    "        },\n",
    "        'Part 4: Analysis': {\n",
    "            'Q4.1': _q4_1_score,\n",
    "            'Q4.2': _q4_2_score,\n",
    "            'Total': _q4_1_score + _q4_2_score,\n",
    "            'Max': 20\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    total_score = sum(part['Total'] for part in scores.values())\n",
    "    total_max = sum(part['Max'] for part in scores.values())\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"ASSIGNMENT GRADE REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Student ID: {hashlib.md5(b'student').hexdigest()[:8]}\")\n",
    "    print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for part_name, part_scores in scores.items():\n",
    "        print(f\"\\n{part_name}\")\n",
    "        print(\"-\" * 40)\n",
    "        for q, score in part_scores.items():\n",
    "            if q not in ['Total', 'Max']:\n",
    "                print(f\"  {q}: {score}\")\n",
    "        print(f\"  Subtotal: {part_scores['Total']}/{part_scores['Max']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"FINAL SCORE: {total_score}/{total_max} ({total_score/total_max*100:.1f}%)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Save grade report\n",
    "    grade_report = {\n",
    "        'student_id': hashlib.md5(b'student').hexdigest()[:8],\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'scores': scores,\n",
    "        'total': total_score,\n",
    "        'max': total_max,\n",
    "        'percentage': round(total_score/total_max*100, 2)\n",
    "    }\n",
    "    \n",
    "    with open('grade_report.json', 'w') as f:\n",
    "        json.dump(grade_report, f, indent=2)\n",
    "    \n",
    "    print(\"\\nGrade report saved to: grade_report.json\")\n",
    "    \n",
    "    return grade_report\n",
    "\n",
    "# Run final grading\n",
    "final_grade = calculate_final_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Submission Instructions\n",
    "\n",
    "1. Make sure all cells run without errors\n",
    "2. Save this notebook\n",
    "3. Submit both:\n",
    "   - This completed notebook (`.ipynb` file)\n",
    "   - The generated `grade_report.json` file\n",
    "\n",
    "### Grading Rubric:\n",
    "- 90-100: Excellent - All implementations correct with efficient code\n",
    "- 80-89: Good - Most implementations correct with minor issues\n",
    "- 70-79: Satisfactory - Core concepts understood, some errors\n",
    "- 60-69: Needs Improvement - Significant gaps in understanding\n",
    "- Below 60: Unsatisfactory - Major conceptual errors\n",
    "\n",
    "Good luck! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
